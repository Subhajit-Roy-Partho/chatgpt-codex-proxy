use anyhow::{anyhow, Context, Result};
use clap::Parser;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use std::collections::HashSet;
use uuid::Uuid;
use warp::{Filter, Reply};

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// Port to listen on
    #[arg(short, long, default_value = "8080")]
    port: u16,

    /// Path to Codex auth.json file
    #[arg(long, default_value = "~/.codex/auth.json")]
    auth_path: String,
}

const DEFAULT_ALLOWED_MODELS: &[&str] = &["gpt-5", "gpt-5.2", "gpt-5.3-codex", "gpt-5.2-codex"];

fn load_allowed_models() -> Vec<String> {
    let configured = std::env::var("ALLOWED_MODELS")
        .ok()
        .map(|raw| {
            raw.split(',')
                .map(str::trim)
                .filter(|value| !value.is_empty())
                .map(ToString::to_string)
                .collect::<Vec<String>>()
        })
        .unwrap_or_default();

    let mut seen = HashSet::new();
    let mut deduped = Vec::new();

    let source: Vec<String> = if configured.is_empty() {
        DEFAULT_ALLOWED_MODELS
            .iter()
            .map(|model| (*model).to_string())
            .collect()
    } else {
        configured
    };

    for model in source {
        if seen.insert(model.clone()) {
            deduped.push(model);
        }
    }

    deduped
}

fn build_models_response(allowed_models: &[String]) -> Value {
    let created = chrono::Utc::now().timestamp();
    let models = allowed_models
        .iter()
        .map(|model| {
            json!({
                "id": model,
                "object": "model",
                "created": created,
                "owned_by": "openai"
            })
        })
        .collect::<Vec<Value>>();

    json!({
        "object": "list",
        "data": models
    })
}

fn build_model_not_allowed_response(model: &str, allowed_models: &[String]) -> Value {
    json!({
        "error": {
            "message": format!(
                "Model '{}' is not allowed by this proxy. Allowed models: {}",
                model,
                allowed_models.join(", ")
            ),
            "type": "invalid_request_error",
            "param": "model",
            "code": "model_not_allowed"
        }
    })
}

fn build_proxy_error_response(error: &str) -> Value {
    json!({
        "error": {
            "message": format!("Proxy error: {}", error),
            "type": "proxy_error",
            "code": "internal_error"
        }
    })
}

fn build_invalid_json_response(error: &str) -> Value {
    json!({
        "error": {
            "message": format!("Invalid JSON body: {}", error),
            "type": "invalid_request_error",
            "param": "body",
            "code": "invalid_json"
        }
    })
}

fn json_response(
    status: warp::http::StatusCode,
    body: &Value,
) -> warp::http::Response<warp::hyper::Body> {
    let reply = warp::reply::with_status(warp::reply::json(body), status);
    let reply = warp::reply::with_header(reply, "content-type", "application/json");
    let reply = warp::reply::with_header(reply, "access-control-allow-origin", "*");
    reply.into_response()
}

/// Chat Completions API format (what CLINE sends)
#[derive(Deserialize, Debug)]
struct ChatCompletionsRequest {
    model: String,
    messages: Vec<ChatMessage>,
    temperature: Option<f32>,
    max_tokens: Option<i32>,
    stream: Option<bool>,
    tools: Option<Vec<Value>>,
    tool_choice: Option<Value>,
}

#[derive(Deserialize, Debug)]
struct ChatMessage {
    role: String,
    content: Value, // Can be string or array
}

/// Chat Completions API response format (what CLINE expects)
#[derive(Serialize, Debug)]
struct ChatCompletionsResponse {
    id: String,
    object: String,
    created: i64,
    model: String,
    choices: Vec<Choice>,
    usage: Option<Usage>,
}

#[derive(Serialize, Debug)]
struct Choice {
    index: i32,
    message: ChatResponseMessage,
    finish_reason: Option<String>,
}

#[derive(Serialize, Debug)]
struct ChatResponseMessage {
    role: String,
    content: String,
}

#[derive(Serialize, Debug)]
struct Usage {
    prompt_tokens: i32,
    completion_tokens: i32,
    total_tokens: i32,
}

/// Codex Responses API format (what we send to ChatGPT backend)
#[derive(Serialize, Debug)]
struct ResponsesApiRequest {
    model: String,
    instructions: String,
    input: Vec<ResponseItem>,
    tools: Vec<Value>,
    tool_choice: String,
    parallel_tool_calls: bool,
    reasoning: Option<Value>,
    store: bool,
    stream: bool,
    include: Vec<String>,
}

#[derive(Serialize, Debug)]
#[serde(tag = "type", rename_all = "snake_case")]
enum ResponseItem {
    Message {
        #[serde(skip_serializing_if = "Option::is_none")]
        id: Option<String>,
        role: String,
        content: Vec<ContentItem>,
    },
}

#[derive(Serialize, Debug)]
#[serde(tag = "type", rename_all = "snake_case")]
enum ContentItem {
    InputText { text: String },
}

/// Codex auth.json structure
#[derive(Deserialize, Debug, Clone)]
struct AuthData {
    #[serde(rename = "OPENAI_API_KEY")]
    api_key: Option<String>,
    tokens: Option<TokenData>,
}

#[derive(Deserialize, Debug, Clone)]
struct TokenData {
    access_token: String,
    account_id: String,
    refresh_token: Option<String>,
}

/// Codex Responses API response format
#[derive(Deserialize, Debug)]
struct ResponsesApiResponse {
    response: Option<ResponseOutput>,
    id: Option<String>,
}

#[derive(Deserialize, Debug)]
struct ResponseOutput {
    content: Option<Vec<ResponseContentItem>>,
    role: Option<String>,
}

#[derive(Deserialize, Debug)]
struct ResponseContentItem {
    #[serde(rename = "type")]
    content_type: String,
    text: Option<String>,
}

struct ProxyServer {
    client: Client,
    auth_data: AuthData,
    allowed_models: Vec<String>,
}

impl ProxyServer {
    async fn new(auth_path: &str) -> Result<Self> {
        let auth_path = if auth_path.starts_with("~/") {
            let home = std::env::var("HOME").context("HOME environment variable not set")?;
            auth_path.replace("~", &home)
        } else {
            auth_path.to_string()
        };

        let auth_content = tokio::fs::read_to_string(&auth_path)
            .await
            .context("Failed to read auth.json")?;

        let auth_data: AuthData =
            serde_json::from_str(&auth_content).context("Failed to parse auth.json")?;

        // Create client with browser-like configuration
        let client = Client::builder()
            .user_agent("Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            .build()
            .context("Failed to create HTTP client")?;

        let allowed_models = load_allowed_models();
        if allowed_models.is_empty() {
            return Err(anyhow!(
                "No allowed models configured. Set ALLOWED_MODELS or use defaults."
            ));
        }

        Ok(Self {
            client,
            auth_data,
            allowed_models,
        })
    }

    fn allowed_models(&self) -> &[String] {
        &self.allowed_models
    }

    fn is_model_allowed(&self, model: &str) -> bool {
        self.allowed_models.iter().any(|allowed| allowed == model)
    }

    fn models_response(&self) -> Value {
        build_models_response(self.allowed_models())
    }

    fn convert_chat_to_responses(&self, chat_req: ChatCompletionsRequest) -> ResponsesApiRequest {
        // Convert messages to ResponseItems
        let mut input = Vec::new();

        for msg in chat_req.messages {
            // Convert content to string (handle both string and array formats)
            let content_text = match &msg.content {
                Value::String(s) => s.clone(),
                Value::Array(arr) => {
                    // Extract text from array elements
                    arr.iter()
                        .filter_map(|v| {
                            if let Some(obj) = v.as_object() {
                                obj.get("text")
                                    .and_then(|t| t.as_str())
                                    .map(|s| s.to_string())
                            } else {
                                v.as_str().map(|s| s.to_string())
                            }
                        })
                        .collect::<Vec<String>>()
                        .join(" ")
                }
                _ => msg.content.to_string(),
            };

            input.push(ResponseItem::Message {
                id: None,
                role: msg.role,
                content: vec![ContentItem::InputText { text: content_text }],
            });
        }

        // Use proper instructions for ChatGPT Responses API
        let instructions = "You are a helpful AI assistant. Provide clear, accurate, and concise responses to user questions and requests.".to_string();

        ResponsesApiRequest {
            model: chat_req.model,
            instructions,
            input,
            tools: chat_req.tools.unwrap_or_default(),
            tool_choice: "auto".to_string(),
            parallel_tool_calls: false,
            reasoning: None,
            store: false,
            stream: true,
            include: vec![],
        }
    }

    async fn proxy_request(
        &self,
        chat_req: ChatCompletionsRequest,
    ) -> Result<ChatCompletionsResponse> {
        println!("ðŸ”„ Processing proxy request...");
        self.proxy_request_original(chat_req).await
    }

    async fn proxy_request_original(
        &self,
        chat_req: ChatCompletionsRequest,
    ) -> Result<ChatCompletionsResponse> {
        // Convert to Responses API format
        let responses_req = self.convert_chat_to_responses(chat_req);

        // Build request to ChatGPT backend with browser-like headers
        let mut request_builder = self
            .client
            .post("https://chatgpt.com/backend-api/codex/responses")
            .header("Content-Type", "application/json")
            .header("Accept", "text/event-stream")
            .header("Accept-Language", "en-US,en;q=0.9")
            .header("Accept-Encoding", "gzip, deflate, br")
            .header("Referer", "https://chatgpt.com/")
            .header("Origin", "https://chatgpt.com")
            .header("Sec-Fetch-Dest", "empty")
            .header("Sec-Fetch-Mode", "cors")
            .header("Sec-Fetch-Site", "same-origin")
            .header("Cache-Control", "no-cache")
            .header("Pragma", "no-cache")
            .header("DNT", "1")
            .header("OpenAI-Beta", "responses=experimental")
            .header("originator", "codex_cli_rs");

        // Add authentication
        if let Some(tokens) = &self.auth_data.tokens {
            request_builder =
                request_builder.header("Authorization", format!("Bearer {}", tokens.access_token));
            request_builder = request_builder.header("chatgpt-account-id", &tokens.account_id);
        } else if let Some(api_key) = &self.auth_data.api_key {
            request_builder =
                request_builder.header("Authorization", format!("Bearer {}", api_key));
        }

        // Add session ID
        let session_id = Uuid::new_v4();
        request_builder = request_builder.header("session_id", session_id.to_string());

        // Send request
        let response = request_builder
            .json(&responses_req)
            .send()
            .await
            .context("Failed to send request to ChatGPT backend")?;

        if !response.status().is_success() {
            let status = response.status();
            let body = response.text().await.unwrap_or_default();
            return Err(anyhow!(
                "ChatGPT backend returned {} with body: {}",
                status,
                body
            ));
        }

        // Handle streaming response
        let mut response_content = String::new();
        let mut fallback_output_text = String::new();
        let mut saw_delta = false;
        let response_text = response.text().await?;
        let lines: Vec<&str> = response_text.lines().collect();

        for line in lines {
            if line.starts_with("data: ") {
                let json_data = &line[6..]; // Remove "data: " prefix
                if json_data == "[DONE]" {
                    break;
                }

                if let Ok(event) = serde_json::from_str::<serde_json::Value>(json_data) {
                    if let Some(event_type) = event.get("type").and_then(|v| v.as_str()) {
                        match event_type {
                            "response.output_text.delta" => {
                                if let Some(delta) = event.get("delta").and_then(|v| v.as_str()) {
                                    saw_delta = true;
                                    response_content.push_str(delta);
                                }
                            }
                            "response.output_item.done" => {
                                if let Some(item) = event.get("item") {
                                    if let Some(content_arr) =
                                        item.get("content").and_then(|v| v.as_array())
                                    {
                                        for content_item in content_arr {
                                            if let Some(text) =
                                                content_item.get("text").and_then(|v| v.as_str())
                                            {
                                                fallback_output_text.push_str(text);
                                            }
                                        }
                                    }
                                }
                            }
                            _ => {} // Ignore other event types
                        }
                    }
                }
            }
        }

        if !saw_delta && !fallback_output_text.is_empty() {
            response_content = fallback_output_text;
        }

        // If no content was collected, surface an explicit error instead of faking output.
        if response_content.is_empty() {
            return Err(anyhow!(
                "ChatGPT backend returned success but no assistant content could be extracted"
            ));
        }

        // Create Chat Completions response
        let chat_res = ChatCompletionsResponse {
            id: format!("chatcmpl-{}", Uuid::new_v4()),
            object: "chat.completion".to_string(),
            created: chrono::Utc::now().timestamp(),
            model: responses_req.model.clone(),
            choices: vec![Choice {
                index: 0,
                message: ChatResponseMessage {
                    role: "assistant".to_string(),
                    content: response_content,
                },
                finish_reason: Some("stop".to_string()),
            }],
            usage: Some(Usage {
                prompt_tokens: 0,
                completion_tokens: 0,
                total_tokens: 0,
            }),
        };

        Ok(chat_res)
    }
}

// Enhanced logging function
fn log_request(method: &warp::http::Method, path: &str, headers: &warp::http::HeaderMap) {
    let timestamp = chrono::Utc::now().format("%Y-%m-%d %H:%M:%S%.3f UTC");

    println!("\nðŸ” === INTERCEPTED REQUEST ===");
    println!("â° Timestamp: {}", timestamp);
    println!("ðŸ“¥ Method: {}", method);
    println!("ðŸ“ Path: {}", path);

    // Log all headers with special attention to problematic ones
    println!("\nðŸ“‹ Headers ({} total):", headers.len());
    for (name, value) in headers.iter() {
        let header_name = name.as_str().to_lowercase();
        let value_str = match value.to_str() {
            Ok(v) => v,
            Err(_) => "[INVALID UTF-8]",
        };

        // Highlight potential CLINE-specific headers
        if header_name.contains("user-agent")
            || header_name.contains("client")
            || header_name.contains("cline")
        {
            println!("  ðŸŽ¯ {}: {}", name, value_str);
        } else if header_name == "authorization" {
            println!(
                "  ðŸ” {}: {}***",
                name,
                &value_str[..std::cmp::min(20, value_str.len())]
            );
        } else {
            println!("  ðŸ“„ {}: {}", name, value_str);
        }
    }

    // Check for VS Code specific patterns
    let user_agent = headers
        .get("user-agent")
        .and_then(|v| v.to_str().ok())
        .unwrap_or("none");

    if user_agent.to_lowercase().contains("vscode") {
        println!("ðŸŽ¯ DETECTED: VS Code client!");
    }
    if user_agent.to_lowercase().contains("cline") {
        println!("ðŸŽ¯ DETECTED: CLINE extension!");
    }

    println!("ðŸ” === END INTERCEPT ===\n");
}

// Removed catch_all_handler - using inline closure to avoid body consumption conflicts

#[tokio::main]
async fn main() -> Result<()> {
    env_logger::init();
    let args = Args::parse();

    println!("Initializing Codex OpenAI Proxy...");

    let proxy = ProxyServer::new(&args.auth_path).await?;
    println!("âœ“ Loaded authentication from {}", args.auth_path);
    println!("âœ“ Allowed models: {}", proxy.allowed_models().join(", "));

    // Multiple endpoints for CLINE compatibility
    let allowed_models_display = proxy.allowed_models().join(", ");
    let proxy_filter = warp::any().map(move || proxy.clone());

    // CORS headers - allow all headers to fix CLINE issues
    let cors = warp::cors()
        .allow_any_origin()
        .allow_headers(vec![
            "authorization",
            "content-type",
            "accept",
            "accept-encoding",
            "x-stainless-arch",
            "x-stainless-lang",
            "x-stainless-os",
            "x-stainless-package-version",
            "x-stainless-retry-count",
            "x-stainless-runtime",
            "x-stainless-runtime-version",
            "x-stainless-timeout",
        ])
        .allow_methods(vec!["GET", "POST", "PUT", "DELETE", "OPTIONS"]);

    // BULLETPROOF SOLUTION - Single universal handler (removed old catch_all)
    let universal_handler = warp::any()
        .and(warp::method())
        .and(warp::path::full())
        .and(warp::header::headers_cloned())
        .and(warp::body::bytes())
        .and(proxy_filter.clone())
        .and_then(universal_request_handler);

    let routes = universal_handler.with(cors).with(warp::log("codex_proxy"));

    println!(
        "ðŸš€ Codex OpenAI Proxy listening on http://0.0.0.0:{}",
        args.port
    );
    println!("   Health check: http://localhost:{}/health", args.port);
    println!(
        "   Chat endpoint: http://localhost:{}/v1/chat/completions",
        args.port
    );
    println!("\n   Configure CLINE with:");
    println!("   Base URL: http://localhost:{}", args.port);
    println!("   Allowed Models: {}", allowed_models_display);
    println!("   API Key: (any value)");

    warp::serve(routes).run(([0, 0, 0, 0], args.port)).await;

    Ok(())
}

// Universal handler that routes based on path and method
async fn universal_request_handler(
    method: warp::http::Method,
    path: warp::path::FullPath,
    headers: warp::http::HeaderMap,
    body: bytes::Bytes,
    proxy: ProxyServer,
) -> Result<impl warp::Reply, warp::Rejection> {
    let path_str = path.as_str();

    log_request(&method, path_str, &headers);

    match (method.as_str(), path_str) {
        ("GET", "/health") => {
            println!("ðŸ’š Health check requested");
            Ok(warp::reply::json(&json!({
                "status": "ok",
                "service": "codex-openai-proxy"
            }))
            .into_response())
        }
        ("GET", "/models") | ("GET", "/v1/models") => {
            println!("ðŸ“‹ === MATCHED MODELS REQUEST ===");
            println!("ðŸ“‹ === END MATCHED ===\n");

            let models_response = proxy.models_response();
            Ok(warp::reply::json(&models_response).into_response())
        }
        ("POST", "/chat/completions") | ("POST", "/v1/chat/completions") => {
            println!("ðŸ”¥ === MATCHED CHAT COMPLETIONS ===");

            // LOG EXACT CLINE REQUEST FOR CURL REPLICATION
            println!("\nðŸ“‹ === CLINE REQUEST DETAILS FOR CURL ===");
            println!("Method: POST");
            println!("Path: {}", path_str);
            println!("Body size: {} bytes", body.len());

            // Log all headers in curl format
            println!("\nHeaders for curl:");
            for (name, value) in headers.iter() {
                if let Ok(value_str) = value.to_str() {
                    if name.as_str().to_lowercase() == "authorization" {
                        println!(
                            "  -H \"{}: {}***\"",
                            name,
                            &value_str[..std::cmp::min(20, value_str.len())]
                        );
                    } else if name.as_str().to_lowercase().starts_with("x-forwarded") {
                        println!("  # Skip: -H \"{}: {}\"", name, value_str);
                    } else {
                        println!("  -H \"{}: {}\"", name, value_str);
                    }
                }
            }

            // Log body (truncated for readability)
            println!("\nBody (first 1000 chars):");
            if let Ok(body_str) = std::str::from_utf8(&body) {
                let truncated = if body_str.len() > 1000 {
                    format!("{}... [TRUNCATED]", &body_str[..1000])
                } else {
                    body_str.to_string()
                };
                println!("{}", truncated);

                // Generate curl command
                println!("\nðŸš€ CURL COMMAND TO REPLICATE:");
                println!("curl -X POST http://localhost:8888{} \\", path_str);
                for (name, value) in headers.iter() {
                    if let Ok(value_str) = value.to_str() {
                        if !name.as_str().to_lowercase().starts_with("x-forwarded")
                            && name.as_str().to_lowercase() != "host"
                        {
                            if name.as_str().to_lowercase() == "authorization" {
                                println!("  -H \"{}: test-key\" \\", name);
                            } else {
                                println!("  -H \"{}: {}\" \\", name, value_str);
                            }
                        }
                    }
                }
                println!("  -d '{}'", body_str.chars().take(500).collect::<String>());
            }
            println!("ðŸ“‹ === END CLINE REQUEST DETAILS ===\n");

            // Parse JSON from bytes
            let chat_req: ChatCompletionsRequest = match serde_json::from_slice(&body) {
                Ok(req) => req,
                Err(e) => {
                    println!("âŒ JSON parse error: {}", e);
                    return Ok(json_response(
                        warp::http::StatusCode::BAD_REQUEST,
                        &build_invalid_json_response(&e.to_string()),
                    ));
                }
            };

            if !proxy.is_model_allowed(&chat_req.model) {
                return Ok(json_response(
                    warp::http::StatusCode::BAD_REQUEST,
                    &build_model_not_allowed_response(&chat_req.model, proxy.allowed_models()),
                ));
            }

            println!("   Model: {}", chat_req.model);
            println!("   Messages: {} items", chat_req.messages.len());
            for (i, msg) in chat_req.messages.iter().enumerate() {
                let content_preview = match &msg.content {
                    Value::String(s) => s.chars().take(50).collect::<String>(),
                    Value::Array(arr) => format!("[array with {} items]", arr.len()),
                    _ => format!(
                        "[{}]",
                        msg.content.to_string().chars().take(50).collect::<String>()
                    ),
                };
                println!("   [{}] {}: {}", i, msg.role, content_preview);
            }
            println!("ðŸ”¥ === END MATCHED ===\n");

            // Check if streaming is requested
            if chat_req.stream.unwrap_or(false) {
                println!("ðŸ”„ STREAMING: CLINE requested streaming response");

                match proxy.proxy_request(chat_req).await {
                    Ok(response) => {
                        let chunk_id = format!("chatcmpl-{}", Uuid::new_v4());
                        let model = response.model.clone();
                        let message = response
                            .choices
                            .first()
                            .map(|choice| choice.message.content.clone())
                            .unwrap_or_default();
                        let message_json =
                            serde_json::to_string(&message).unwrap_or_else(|_| "\"\"".to_string());

                        let sse_chunks = vec![
                            format!(
                                "data: {{\"id\":\"{}\",\"object\":\"chat.completion.chunk\",\"created\":{},\"model\":\"{}\",\"choices\":[{{\"index\":0,\"delta\":{{\"role\":\"assistant\"}},\"finish_reason\":null}}]}}\n\n",
                                chunk_id,
                                chrono::Utc::now().timestamp(),
                                model
                            ),
                            format!(
                                "data: {{\"id\":\"{}\",\"object\":\"chat.completion.chunk\",\"created\":{},\"model\":\"{}\",\"choices\":[{{\"index\":0,\"delta\":{{\"content\":{}}},\"finish_reason\":null}}]}}\n\n",
                                chunk_id,
                                chrono::Utc::now().timestamp(),
                                model,
                                message_json
                            ),
                            format!(
                                "data: {{\"id\":\"{}\",\"object\":\"chat.completion.chunk\",\"created\":{},\"model\":\"{}\",\"choices\":[{{\"index\":0,\"delta\":{{}},\"finish_reason\":\"stop\"}}]}}\n\n",
                                chunk_id,
                                chrono::Utc::now().timestamp(),
                                model
                            ),
                            "data: [DONE]\n\n".to_string(),
                        ];

                        let sse_response = sse_chunks.join("");
                        let reply = warp::reply::with_header(
                            sse_response,
                            "content-type",
                            "text/event-stream",
                        );
                        let reply = warp::reply::with_header(reply, "cache-control", "no-cache");
                        let reply = warp::reply::with_header(reply, "connection", "keep-alive");
                        let reply =
                            warp::reply::with_header(reply, "access-control-allow-origin", "*");
                        Ok(reply.into_response())
                    }
                    Err(e) => {
                        eprintln!("Proxy error: {:#}", e);
                        let reply = json_response(
                            warp::http::StatusCode::BAD_GATEWAY,
                            &build_proxy_error_response(&e.to_string()),
                        );
                        Ok(reply)
                    }
                }
            } else {
                match proxy.proxy_request(chat_req).await {
                    Ok(response) => {
                        let reply = warp::reply::json(&response);
                        let reply =
                            warp::reply::with_header(reply, "content-type", "application/json");
                        let reply =
                            warp::reply::with_header(reply, "access-control-allow-origin", "*");
                        Ok(reply.into_response())
                    }
                    Err(e) => {
                        eprintln!("Proxy error: {:#}", e);
                        let reply = json_response(
                            warp::http::StatusCode::BAD_GATEWAY,
                            &build_proxy_error_response(&e.to_string()),
                        );
                        Ok(reply)
                    }
                }
            }
        }
        _ => {
            println!("âŒ UNMATCHED: {} {}", method, path_str);
            Ok(
                warp::reply::with_status("Not found", warp::http::StatusCode::NOT_FOUND)
                    .into_response(),
            )
        }
    }
}

// Make ProxyServer cloneable for warp filters
impl Clone for ProxyServer {
    fn clone(&self) -> Self {
        Self {
            client: self.client.clone(),
            auth_data: self.auth_data.clone(),
            allowed_models: self.allowed_models.clone(),
        }
    }
}
